# environment the application is running
hadoop.jobtracker=c3master2-vh.gbif.org:8032
hdfs.namenode=hdfs://ha-nn
oozie.url=http://c3oozie.gbif-dev.org:11000/oozie

# Uses oozies shared lib and the /lib folder in the workflow
oozie.use.system.libpath=true

# hdfs because we do things like chown hbase:hbase
user.name=hdfs

# location of the workflow and jars
#oozie.wf.application.path=hdfs://ha-nn/maps-backfill-workflow
oozie.coord.application.path=hdfs://ha-nn/maps-backfill-workflow/coordinator-points.xml
oozie.libpath=hdfs://ha-nn/maps-backfill-workflow/lib/

# fine tune the spark submission
gbif.map.spark.opts=--driver-memory 2G --executor-memory 6G --num-executors 10 --executor-cores 5 --conf spark.yarn.executor.memoryOverhead=4096

# the name of the file for the configuration
gbif.map.spark.conf=dev.yml

# ZK Quorum for the HBase cluster
gbif.map.zk.quorum=c3zk1.gbif-dev.org:2181,c3zk2.gbif-dev.org:2181,c3zk3.gbif-dev.org:2181

# Source HBase occurrence table to snapshot for the input
gbif.map.sourceTable=dev_occurrence

# The target table is finalised as prefix_mode_timestamp (e.g. dev_maps_tiles_201707_01_1315
gbif.map.targetTablePrefix=dev_maps

# The target directory for the HFiles
gbif.map.targetDirectory=hdfs://ha-nn/tmp/dev_maps

# Whether to run the point or the tile backfill
gbif.map.mode=points
#gbif.map.mode=tiles

# The modulus controls the number of regions in the target table (sensible defaults are 5-10 for dev and 100 for prod)
gbif.map.keySaltModulus=100

# The ZK Path where the table names are located
gbif.map.zk.metadataPath=/dev_maps/meta
